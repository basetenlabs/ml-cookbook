# OCaml Specialist 

The recipe here provides a way to train a model code OCaml. The reward function is simple: it returns a 1 for code that compiles, and a 0 for code that doesn't. While 
this won't produce the most prolific OCaml coder, it serves to demonstrate the power and fleixbility of Baseten's training platform, allowing you to instrument
environments with nontrivial dependencies to train novel models. 

## Training Configuration Details 

The configuration has special properties that lead to better rewards in the training run:
* Both runs implement [Truncated Importance Sampling](https://fengyao.notion.site/off-policy-rl)
* The LoRA run is a faithful adaptation of Lora-without-regret using VeRL, specifically with rank = 8. We're able to reach the reward of the full finetune within one epoch. Neither example has been thoroughly swept, and the examples serve more of a jumping off point to allow you to tune for your own use case.


## Examples 

Below is a comparison of the base model and finetuned model, generated by [api_eval.py](./qwen3-8b-lora-verl/api_eval.py)
```
==== Prompt ====
(**Write a function to check if a string is present as a substring in a given list of string values.
*)
let find_substring (str1 : string list) (sub_str : string) : bool =

==== Base Model ====
Solution: 
let find_substring (str1 : string list) (sub_str : string) : bool =
  List.exists (fun s -> String.contains s sub_str) str1

Compiles: False

==== Finetuned LoRA ====
Solution: 
let find_substring (str1 : string list) (sub_str : string) : bool =
  let len_sub = String.length sub_str in
  List.exists (fun s ->
    let len_s = String.length s in
    if len_sub > len_s then false
    else
      let rec loop i =
        if i > len_s - len_sub then false
        else if String.sub s i len_sub = sub_str then true
        else loop (i + 1)
      in
      loop 0
  ) str1

Compiles: True
````

## Running the example

### Install `truss` 
Use the appropriate command for your package manager
```bash
# pip
pip install -U truss
# uv
uv add truss && uv sync --upgrade-package truss
```

### Create the workspace for your training project

```bash
# for the full finetune
truss train init --examples qwen3-8b-fft-verl && cd qwen3-8b-fft-verl

# for the lora example
truss train init --examples qwen3-8b-lora-verl && cd qwen3-8b-lora-verl 
```

### Kick off the job

Make sure you've plugged in proper secrets (e.g. wandb api key, huggingface token) via Baseten Secrets and Environment Variables, and kick off your job

```bash
truss train push config.py
```

For more details, take a look at the [docs](https://docs.baseten.co/training/overview)

