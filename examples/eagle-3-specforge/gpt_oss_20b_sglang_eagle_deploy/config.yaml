apply_library_patches: true
base_image:
  docker_auth: null
  image: lmsysorg/sglang:latest
  python_executable_path: ''
build:
  arguments: {}
  model_server: TrussServer
  secret_to_path_mapping: {}
bundled_packages_dir: packages
cache_internal: []
data_dir: data
description: Baseten config for GPT-OSS-20B baseline model using vLLM
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  readiness_endpoint: /health
  server_port: 8000
  start_command: '%(ENV_BT_DOCKER_SERVER_START_CMD)s'
environment_variables:
  BT_DOCKER_SERVER_START_CMD: sh -c "chmod +x /app/model/run.sh && /app/model/run.sh"
  HF_HUB_ENABLE_HF_TRANSFER: '1'
  VLLM_LOGGING_LEVEL: WARNING
examples_filename: examples.yaml
external_data: null
external_package_dirs: []
input_type: Any
live_reload: false
model_cache: []
model_class_filename: model.py
model_class_name: Model
model_framework: custom
model_metadata: {}
model_module_dir: model
model_name: new-format-finetuned-gpt-oss-20b
model_type: Model
python_version: py39
requirements: []
requirements_file: null
resources:
  accelerator: H100:1
  cpu: '2'
  memory: 30Gi
  node_count: 1
  use_gpu: true
runtime:
  enable_debug_logs: false
  enable_tracing_data: false
  health_checks:
    restart_check_delay_seconds: null
    restart_threshold_seconds: null
    stop_traffic_threshold_seconds: null
  is_websocket_endpoint: false
  predict_concurrency: 64
  streaming_read_timeout: 60
  transport:
    kind: http
  truss_server_version_override: null
secrets:
  hf_access_token: set token in baseten workspace
spec_version: '2.0'
system_packages: []
training_checkpoints: null
trt_llm: null
use_local_src: false
