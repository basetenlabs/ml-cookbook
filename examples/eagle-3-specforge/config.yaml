model_name: Qwen3-4B-Eagle3
build_commands:
  - hf download baseten-admin/model-eagle-head
python_version: py311
model_metadata:
  repo_id: Qwen/Qwen3-4B
  example_model_input:
    {
      "model": "Qwen/Qwen3-4B",
      "messages":
        [
          {
            "role": "user",
            "content": "What is Qwen3-4B and what are some applications of this model?",
          },
        ],
      "stream": true,
      "max_tokens": 2048,
      "temperature": 0.5,
    }
  tags:
    - openai-compatible
resources:
  accelerator: H100:2
  cpu: "2"
  memory: 10Gi
  use_gpu: true
model_cache:
  - repo_id: baseten-admin/model-eagle-head
    revision: main
    use_volume: true
    volume_folder: qwen3_4b_eagle3
trt_llm:
  build:
    checkpoint_repository:
      repo: huihui-ai/Huihui-Qwen3-4B-abliterated-v2 # example checkpoint, please change this
      revision: main
      source: HF
  inference_stack: v2
  runtime:
    enable_chunked_prefill: true
    max_batch_size: 64
    max_num_tokens: 2048
    max_seq_len: 2048
    tensor_parallel_size: 2
    patch_kwargs:
      disable_overlap_scheduler: True
      cuda_graph_config:
        enable_padding: true
      guided_decoding_backend: null
      kv_cache_config:
        enable_block_reuse: true
        free_gpu_memory_fraction: 0.8
      speculative_config:
        decoding_type: Eagle
        max_draft_len: 3
        speculative_model_dir: /app/model_cache/qwen3_4b_eagle3
