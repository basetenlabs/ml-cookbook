model_name: GPT OSS 20B
build_commands:
  - python -c 'from openai_harmony import load_harmony_encoding; load_harmony_encoding("HarmonyGptOss")'
python_version: py39
model_metadata:
  repo_id: openai/gpt-oss-20b
  example_model_input:
    {
      "model": "openai/gpt-oss-20b",
      "messages":
        [
          {
            "role": "user",
            "content": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order. class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]:",
          },
        ],
      "stream": true,
      "max_tokens": 2048,
      "temperature": 0.5,
    }
  tags:
    - openai-compatible
resources:
  accelerator: H100:2
  cpu: "2"
  memory: 10Gi
  use_gpu: true
model_cache:
  - repo_id: baseten-admin/gpt_lora_oss_20b_eagle_head
    revision: main
    use_volume: true
    volume_folder: gpt_oss_20b_eagle3
trt_llm:
  build:
    checkpoint_repository:
      repo: baseten-admin/gpt_lora_oss_20b
      revision: main
      source: HF
  inference_stack: v2
  runtime:
    enable_chunked_prefill: true
    max_batch_size: 64
    max_num_tokens: 8192
    max_seq_len: 8192
    tensor_parallel_size: 2
    patch_kwargs:
      disable_overlap_scheduler: True
      cuda_graph_config:
        enable_padding: true
      guided_decoding_backend: null
      kv_cache_config:
        enable_block_reuse: true
        free_gpu_memory_fraction: 0.8
      speculative_config:
        decoding_type: Eagle
        max_draft_len: 3
        speculative_model_dir: /app/model_cache/gpt-oss-20b-eagle3
